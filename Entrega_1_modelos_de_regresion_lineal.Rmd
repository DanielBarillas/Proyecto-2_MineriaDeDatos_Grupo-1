---
title: "Entrega 1. Modelos de regresión lineal"
author: 
  - "Pablo Daniel Barillas Moreno, Carné No. 22193"
  - "Mathew Cordero Aquino, Carné No. 22982"
output:
  html_document: default
  pdf_document: default
date: "2025-02-02"
header-includes:
   - \usepackage{fvextra}
   - \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
repository: "https://github.com/DanielBarillas/Proyecto-2_MineriaDeDatos_Grupo-1.git"
---

### Enlace al Repositorio del proyecto 2 de minería de datos del Grupo #1
[Repositorio en GitHub](https://github.com/DanielBarillas/Proyecto-2_MineriaDeDatos_Grupo-1.git)

### 1. Descargue los conjuntos de datos. 

```{r}
train_data <- read.csv("house_prices_data/train.csv", stringsAsFactors = FALSE)
test_data <- read.csv("house_prices_data/test.csv", stringsAsFactors = FALSE)

head(train_data)   # Muestra las primeras filas
str(train_data)    # Muestra la estructura del dataset
summary(train_data) # Resumen estadístico
```

### 2. Haga un análisis exploratorio extenso de los datos. Explique bien todos los hallazgos. No ponga solo gráficas y código. Debe llegar a conclusiones interesantes para poder predecir. Explique el preprocesamiento que necesitó hacer.

Análisis Exploratorio de Datos (EDA)

El objetivo de este análisis es entender la estructura de los datos, identificar patrones, detectar valores atípicos y preparar el dataset para su uso en modelos de regresión. Vamos a explorar el conjunto de datos train.csv.

El análisis exploratorio de datos (EDA) es fundamental para entender las características del dataset, identificar patrones y decidir qué variables serán útiles para la predicción del precio de las viviendas.

2.1. Cargar y Revisar los Datos...

Primero, cargamos los datos e instalamos y cargamos las librerías necesarias y revisamos su estructura.

```{r}
# Instalar paquetes si no están instalados

# Cargar librerías
library(tidyverse)
library(corrplot)
library(VIM)        # Para analizar valores faltantes
library(ggplot2)
library(dplyr)
library(caret)      # Para dividir los datos en entrenamiento y prueba

# Cargar los datos
train <- read_csv("house_prices_data/train.csv")
test <- read_csv("house_prices_data/test.csv")

# Ver la estructura de los datos
str(train)
```

Observaciones

  1. Carga de Datos

  2. Comprensión del Dataset

- El dataset "House Prices: Advanced Regression Techniques" contiene 1460 registros y 81 columnas. La variable objetivo que queremos predecir es SalePrice (el precio de las casas en dólares).

  1.1 Tipos de Variables

- Variables Numéricas: Representan cantidades medibles, como el área de la casa (GrLivArea), el número de baños (FullBath), etc.
- Variables Categóricas: Representan características como el vecindario (Neighborhood), tipo de calle (Street), tipo de fundación (Foundation), etc.

  1.2 Variables Importantes en el Dataset

Algunas variables que podrían influir en el precio de una casa incluyen:

- Ubicación: Neighborhood
- Tamaño: GrLivArea, LotArea
- Calidad de Construcción: OverallQual, OverallCond
- Baños y Habitaciones: FullBath, Bedroom
- Garaje y Estacionamiento: GarageCars, GarageArea
- Edad de la Casa: YearBuilt, YearRemodAdd

2. Detalles importantes

- Se cargaron los archivos train.csv (1460 filas, 81 columnas) y test.csv (1459 filas, 80 columnas).
- La estructura de los datos muestra que hay variables numéricas (dbl) y categóricas (chr).
- SalePrice es la variable objetivo y tiene valores numéricos.

- Conclusión: La carga de datos fue exitosa y el dataset tiene una combinación de variables numéricas y categóricas.

3. Valores Faltantes

- Variables como Alley, PoolQC, Fence, MiscFeature, FireplaceQu tienen muchos valores faltantes (más del 80%).
- Otras variables como LotFrontage, GarageYrBlt, MasVnrArea, BsmtQual, BsmtCond, BsmtExposure también tienen valores faltantes, pero en menor cantidad.

- Conclusión:
Variables con más del 80% de valores faltantes deben eliminarse (PoolQC, MiscFeature, Alley, Fence, FireplaceQu).
Valores faltantes en variables numéricas pueden imputarse con la mediana (LotFrontage).
Valores faltantes en variables categóricas deben reemplazarse por "None" (GarageType, BsmtQual, etc.).

4. Análisis de Correlación

Se identificó que las variables con mayor correlación con SalePrice son:

- OverallQual (0.79) → Calidad de construcción.
- GrLivArea (0.70) → Área habitable.
- GarageCars (0.64) → Capacidad del garaje.
- TotalBsmtSF (0.61) → Tamaño del sótano.

- Conclusión: Estas variables son claves para predecir el precio de las casas, por lo que se incluirán en la regresión.

5. Distribución de SalePrice
- SalePrice tiene una distribución sesgada a la derecha, lo que puede afectar la regresión.
- Solución: Aplicar una transformación logarítmica log(SalePrice) para normalizar la distribución.
- Conclusión: Se aplicará log(SalePrice) para mejorar la calidad del modelo.

6. Conversión de Variables Categóricas
- Variables categóricas como Neighborhood, HouseStyle, SaleType deben convertirse a factores para usarlas en el modelo.

- Conclusión: Las variables categóricas serán transformadas para que el modelo pueda manejarlas correctamente.


2.2. Inspección Inicial de los Datos

```{r}
# Dimensiones del dataset
dim(train)  # 1460 filas, 81 columnas

# Ver las primeras filas del dataset
head(train)

# Resumen estadístico de las variables numéricas
summary(train)

# Verificar cuántos valores faltantes tiene cada columna
colSums(is.na(train))
```
2.3. Análisis de la Variable Objetivo (SalePrice)

La variable SalePrice representa el precio de venta de las casas. Analicemos su distribución.

```{r}
# Distribución de SalePrice
ggplot(train, aes(x = SalePrice)) + 
  geom_histogram(bins = 50, fill = "blue", alpha = 0.7) +
  labs(title = "Distribución de Precios de Casas", x = "Precio de Venta", y = "Frecuencia")

# Aplicar transformación logarítmica para mejorar normalidad
train$LogSalePrice <- log(train$SalePrice)

# Transformación logarítmica
ggplot(train_data, aes(x = log(SalePrice))) +
  geom_histogram(bins = 50, fill = "red", alpha = 0.7) +
  labs(title = "Distribución Logarítmica de los Precios", x = "Log(Precio)", y = "Frecuencia")

# Comparación antes y después de la transformación
par(mfrow = c(1, 2))
hist(train$SalePrice, main = "Distribución de SalePrice", col = "blue", breaks = 50)
hist(train$LogSalePrice, main = "Distribución Log(SalePrice)", col = "red", breaks = 50)
```

Hallazgos

El precio de las casas (SalePrice) no tiene una distribución normal. La mayoría de los precios se concentran en valores bajos y hay algunas casas extremadamente caras que podrían ser outliers.

- SalePrice presenta sesgo positivo (distribución asimétrica a la derecha), lo que indica que hay casas con precios extremadamente altos.
- Posibles valores atípicos en precios muy elevados que podrían afectar el modelo.
- Será útil aplicar una transformación logarítmica para normalizar la distribución.

- Solución: Aplicar una transformación logarítmica para mejorar la normalidad:
log(SalePrice)

2.3.1.  Relación entre Variables y SalePrice

Para entender qué variables influyen más en el precio de las casas, veamos la correlación de variables numéricas.

```{r}
# Seleccionar solo variables numéricas
numeric_vars <- train_data %>% select_if(is.numeric)

# Matriz de correlación
cor_matrix <- cor(numeric_vars, use = "complete.obs")

# Ordenar correlaciones con SalePrice
cor_with_price <- sort(cor_matrix["SalePrice",], decreasing = TRUE)
cor_with_price
```
Hallazgos

Las variables con mayor correlación positiva con el precio de las casas son:

- OverallQual (Calidad general de la casa) → 0.79 
- GrLivArea (Área habitable sobre el suelo) → 0.71 
- GarageCars (Número de coches en el garaje) → 0.64 
- TotalBsmtSF (Área total del sótano) → 0.61 

Esto sugiere que casas más grandes y con mejor calidad tienden a ser más caras.

Visualización

```{r}
ggplot(train_data, aes(x = OverallQual, y = SalePrice)) +
  geom_boxplot(aes(group = OverallQual, fill = as.factor(OverallQual))) +
  labs(title = "Precio vs Calidad General", x = "Calidad", y = "Precio")
```
Conclusión: La calidad (OverallQual) es una variable crucial para predecir SalePrice.

2.3.2. Detectar Valores Atípicos

Para evitar que valores extremos afecten el modelo, busquemos outliers.

```{r}
ggplot(train_data, aes(x = GrLivArea, y = SalePrice)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", col = "red") +
  labs(title = "Área vs Precio", x = "Área habitable", y = "Precio")
```
Hallazgos

- Se observan dos puntos con GrLivArea > 4000 y precios muy bajos.
- Estas casas son atípicas y podrían eliminarse para mejorar la predicción.

```{r}
# Eliminar outliers
train_data <- train_data[train_data$GrLivArea < 4000, ]
```

2.4. Análisis de Correlación

Antes de construir un modelo, es importante manejar los valores NA.

```{r}
# Seleccionar solo las variables numéricas
num_vars <- train %>% select(where(is.numeric))

# Calcular la matriz de correlación
corr_matrix <- cor(num_vars, use = "complete.obs")

# Visualizar la correlación con SalePrice
corrplot(corr_matrix, method = "color", type = "upper", tl.cex = 0.6)

```
Se analizó la correlación entre las variables numéricas y SalePrice. Las variables con mayor correlación positiva con el precio son:

- OverallQual (0.79): Calidad de materiales y acabados.
- GrLivArea (0.70): Área habitable total.
- TotalBsmtSF (0.61): Área del sótano.
- GarageCars (0.64): Cantidad de autos que caben en el garaje.

- Conclusión:

Estas variables serán claves en la regresión lineal.
Variables con baja o nula correlación (como MiscFeature y PoolArea) probablemente no sean útiles en el modelo.

Las variables con mayor correlación con SalePrice:

- OverallQual (Calidad de la construcción): Es una de las variables más correlacionadas con SalePrice, lo - que confirma que las casas con mejor calidad de construcción tienen precios más altos.
- GrLivArea (Área habitable sobre el nivel del suelo): También tiene una correlación alta con SalePrice, lo que significa que las casas más grandes suelen costar más.
- TotalBsmtSF (Área total del sótano): Muestra una correlación fuerte con SalePrice, lo que implica que un - sótano más grande puede aumentar el valor de la vivienda.
- GarageCars (Capacidad del garaje en número de autos): Tiene una buena correlación con SalePrice, lo que sugiere que tener más espacio de garaje incrementa el valor de la casa.

2.5. Identificar y Manejar Valores Faltantes

```{r}
# Ver cantidad de valores faltantes
missing_values <- colSums(is.na(train))
missing_values <- sort(missing_values[missing_values > 0], decreasing = TRUE)
print(missing_values)
```
Decisiones para valores faltantes:

Eliminar columnas con más del 80% de valores faltantes:

```{r}
train <- train %>% select(-c(PoolQC, MiscFeature, Alley, Fence))
test <- test %>% select(-c(PoolQC, MiscFeature, Alley, Fence))
```

Imputar valores faltantes en LotFrontage con la mediana del vecindario:

```{r}
train$LotFrontage[is.na(train$LotFrontage)] <- median(train$LotFrontage, na.rm = TRUE)
```

Reemplazar valores faltantes en variables categóricas con "None":

```{r}
categorical_vars <- c("GarageType", "BsmtQual", "BsmtCond", "FireplaceQu")
for (var in categorical_vars) {
  train[[var]][is.na(train[[var]])] <- "None"
}
```

Varias columnas tienen valores faltantes. Algunas de las más afectadas son:

- PoolQC (99% de valores faltantes)
- MiscFeature (96% faltantes)
- Alley (93% faltantes)
- Fence (80% faltantes)
- FireplaceQu (50% faltantes)

- Solución Propuesta:
Variables como PoolQC, Alley, MiscFeature y Fence contienen información escasa y pueden ser eliminadas.
Variables como LotFrontage (frente del terreno) pueden completarse con la mediana por vecindario.
Variables categóricas con valores faltantes, como GarageType, se rellenarán con "None" (indicando que no existe).

2.6. Transformación de Variables

Aplicar log(SalePrice) para mejorar la normalidad:

```{r}
train$LogSalePrice <- log(train$SalePrice)
```

Convertir variables categóricas en factores:

```{r}
train <- train %>% mutate(across(where(is.character), as.factor))
test <- test %>% mutate(across(where(is.character), as.factor))
```

2.7. Análisis de Variables Categóricas

```{r}
# Distribución de precios por vecindario
ggplot(train, aes(x = Neighborhood, y = SalePrice)) + 
  geom_boxplot() + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(title = "Precios de Casas por Vecindario", x = "Vecindario", y = "Precio de Venta")
```
Algunas variables categóricas pueden influir en SalePrice:

- Neighborhood tiene variaciones significativas en los precios.
- Exterior1st y Exterior2nd pueden influir según la calidad de los materiales.
- SaleCondition indica si la venta fue "Normal" o una subasta, lo que puede afectar el precio.

- Solución Propuesta:
Convertir variables categóricas a numéricas mediante codificación dummy (One-Hot Encoding).

2.8. División del Conjunto de Datos

Dividimos el dataset en entrenamiento (80%) y prueba (20%):

```{r}
set.seed(42)
trainIndex <- createDataPartition(train$SalePrice, p = 0.8, list = FALSE)
train_set <- train[trainIndex, ]
test_set <- train[-trainIndex, ]
```

Preprocesamiento de Datos

Eliminación de Variables con Demasiados Valores Faltantes

Se eliminaron variables con más del 80% de valores faltantes:

- PoolQC, MiscFeature, Alley, Fence.

Imputación de Valores Faltantes
- Para valores numéricos (LotFrontage): Se imputó con la mediana por vecindario.
- Para valores categóricos (GarageType, BsmtQual): Se reemplazaron con "None".

Transformación Logarítmica de SalePrice

Dado que SalePrice estaba sesgado, aplicamos una transformación logarítmica para mejorar la distribución:

- log(SalePrice)

Conversión de Variables Categóricas

- Las variables categóricas se convirtieron a factores para su uso en la regresión.

Conclusiones

Las variables con más impacto en SalePrice son:

- OverallQual, GrLivArea, TotalBsmtSF, GarageCars.
- Variables categóricas como Neighborhood también afectan el precio.

La distribución de SalePrice está sesgada, por lo que podemos usar logaritmos para mejorar el modelo.

Las variables más importantes para predecir el precio son:

- OverallQual (Calidad general de la casa)
- GrLivArea (Área habitable)
- GarageCars (Cantidad de autos en garaje)
- TotalBsmtSF (Área del sótano)

Valores Faltantes:

- Se eliminaron columnas irrelevantes (PoolQC, Alley).
Hay valores faltantes, principalmente en características poco comunes como piscinas y chimeneas.

- Se imputaron valores para LotFrontage y GarageType.

- Existen valores atípicos en GrLivArea que afectan el análisis y deben eliminarse.

Transformaciones Realizadas:

- Se aplicó logaritmo a SalePrice para mejorar su distribución.
- Variables categóricas se convirtieron en numéricas para el modelo.

### 3. Incluya un análisis de grupos en el análisis exploratorio. Explique las características de cada uno. 

3. Análisis de Grupos en el Análisis Exploratorio

Objetivo:

El análisis de grupos nos permite identificar patrones en los datos y clasificar las casas en segmentos según características comunes. Esto puede ayudar a mejorar la predicción de SalePrice.

3.1 Creación de Grupos Basados en Calidad de Construcción (OverallQual)

Dado que OverallQual tiene una alta correlación con SalePrice, podemos dividir las casas en tres grupos según su calidad:

```{r}
train <- train %>%
  mutate(QualityGroup = case_when(
    OverallQual <= 4 ~ "Baja",
    OverallQual >= 5 & OverallQual <= 7 ~ "Media",
    OverallQual >= 8 ~ "Alta"
  ))

# Visualizar la distribución de precios en los grupos
ggplot(train, aes(x = QualityGroup, y = SalePrice, fill = QualityGroup)) +
  geom_boxplot() +
  labs(title = "Distribución de Precios por Calidad de Construcción",
       x = "Grupo de Calidad", y = "Precio de Venta")
```
Hallazgos

- Las casas de calidad alta (OverallQual ≥ 8) tienen un precio significativamente mayor.
- Las casas de calidad media (OverallQual 5-7) forman la mayoría del dataset y muestran mayor variabilidad en los precios.
- Las casas de calidad baja (OverallQual ≤ 4) tienen precios considerablemente menores.

3.2 Segmentación Basada en Vecindario (Neighborhood)

- Otra forma de agrupar las casas es según la ubicación (Neighborhood), ya que este factor influye en los precios.

```{r}
# Agrupar por vecindario y calcular estadísticas básicas
neighborhood_summary <- train %>%
  group_by(Neighborhood) %>%
  summarise(AvgPrice = mean(SalePrice, na.rm = TRUE),
            MedianPrice = median(SalePrice, na.rm = TRUE),
            Count = n()) %>%
  arrange(desc(AvgPrice))

print(neighborhood_summary)
```
Hallazgos

- Los vecindarios más caros son "StoneBr", "NridgHt", "NoRidge", con precios promedio por encima de 300,000 dólares.
- Los vecindarios más baratos son "MeadowV", "IDOTRR", "BrDale", con precios promedio por debajo de 150,000 dólares.
- Se pueden agrupar los vecindarios en zonas de alto, medio y bajo valor para mejorar la predicción.

3.3 Análisis de Grupos Basados en Tamaño de Casa (GrLivArea)

- Agrupamos las casas en pequeñas, medianas y grandes según el área habitable (GrLivArea):

```{r}
train <- train %>%
  mutate(SizeGroup = case_when(
    GrLivArea < 1000 ~ "Pequeña",
    GrLivArea >= 1000 & GrLivArea < 2000 ~ "Mediana",
    GrLivArea >= 2000 ~ "Grande"
  ))

# Visualizar precios por tamaño de casa
ggplot(train, aes(x = SizeGroup, y = SalePrice, fill = SizeGroup)) +
  geom_boxplot() +
  labs(title = "Distribución de Precios por Tamaño de Casa",
       x = "Tamaño de la Casa", y = "Precio de Venta")
```
Hallazgos

- Las casas grandes (GrLivArea ≥ 2000) tienen precios más altos y mayor variabilidad.
- Las casas medianas (1000 ≤ GrLivArea < 2000) son la mayoría del dataset y presentan una distribución amplia de precios.
- Las casas pequeñas (GrLivArea < 1000) tienen precios bajos y menos variabilidad.

3.4 Análisis de Clustering en el Dataset (Segmentación Automática de Casas)

Objetivo del Clustering:

En lugar de definir manualmente los grupos de casas, aplicamos un método de clustering automático (K-Means) para descubrir patrones en los datos y segmentar las casas en grupos con características similares.

3.4.1. Selección de Variables para Clustering

Seleccionamos variables clave que tienen fuerte relación con SalePrice:

```{r}
# Cargar librerías necesarias
library(cluster)
library(factoextra)
library(dplyr)

# Seleccionar variables más relevantes para clustering
clustering_data <- train %>% select(OverallQual, GrLivArea, TotalBsmtSF, GarageCars)

# Normalizar los datos (evita sesgo por escalas diferentes)
clustering_data <- scale(clustering_data)
```

Las variables seleccionadas (OverallQual, GrLivArea, TotalBsmtSF, GarageCars) reflejan calidad, tamaño y capacidad del garaje, que son fuertes indicadores del precio.

3.4.2. Aplicación del Algoritmo de Clustering (K-Means)

```{r}
# Fijar semilla para reproducibilidad
set.seed(42)

# Aplicar K-Means con 3 clusters (puede ajustarse)
kmeans_result <- kmeans(clustering_data, centers = 3, nstart = 25)

# Agregar los clusters al dataset
train$Cluster <- as.factor(kmeans_result$cluster)
```

Cada casa ha sido asignada a un grupo (Cluster 1, Cluster 2, Cluster 3).
Estos clusters representan diferentes segmentos del mercado inmobiliario.

3.4.3. Visualización de los Clusters

Para analizar cómo se agrupan las casas según el clustering, graficamos la relación entre tamaño (GrLivArea) y precio (SalePrice), coloreando los grupos.

```{r}
ggplot(train, aes(x = GrLivArea, y = SalePrice, color = Cluster)) +
  geom_point(alpha = 0.6) +
  labs(title = "Clusters de Casas basados en Área y Precio",
       x = "Área Habitable (GrLivArea)",
       y = "Precio de Venta (SalePrice)") +
  theme_minimal()
```

Los colores indican los diferentes grupos de casas detectados automáticamente.

3.4.4. Caracterización de los Clusters

Cluster 1 - Casas económicas

- Baja calidad de construcción (OverallQual bajo).
- Tamaño reducido (GrLivArea y TotalBsmtSF pequeños).
- Garaje pequeño o inexistente (GarageCars).
- Bajo SalePrice, generalmente en vecindarios más baratos.

Cluster 2 - Casas de precio medio

- Calidad media-alta (OverallQual entre 5 y 7).
- Tamaño intermedio, con un área habitable moderada.
- Garaje con espacio para 1-2 autos.
- Precio en el rango medio del dataset.

Cluster 3 - Casas de lujo

- Alta calidad de construcción (OverallQual > 7).
- Casas grandes con mucho espacio (GrLivArea alto).
- Garajes amplios (2-3 autos).
- SalePrice alto, típicamente en vecindarios premium.

Gráfica Agrupada por cluster...

```{r}
# Cargar librerías necesarias
library(factoextra)
library(cluster)
library(dplyr)  # Cargar dplyr

# Seleccionar variables para clustering
clustering_data <- train %>% select(OverallQual, GrLivArea, TotalBsmtSF, GarageCars)

# Normalizar los datos (evita sesgo por escalas diferentes)
clustering_data <- scale(clustering_data)

# Aplicar K-Means con 3 clusters
set.seed(42)  # Asegurar reproducibilidad
kmeans_result <- kmeans(clustering_data, centers = 3, nstart = 25)

# Agregar clusters al dataset
train$Cluster <- as.factor(kmeans_result$cluster)

# Visualizar los clusters
fviz_cluster(kmeans_result, data = clustering_data,
             geom = "point", ellipse.type = "norm",
             palette = c("red", "blue", "green"),
             ggtheme = theme_minimal(),
             main = "Cluster plot")
```

Conclusiones del Análisis de Clustering

El gráfico de clustering muestra tres grupos diferenciados en el dataset de precios de casas. Basándonos en la distribución y separación de los clusters, se pueden hacer las siguientes observaciones:

1. Interpretación de la Gráfica Agrupada por cluster

  Cluster 1 (Rojo - Izquierda): Casas de Bajo Precio y Tamaño Pequeño

- Este grupo representa casas con menor calidad de construcción (OverallQual baja), menor área habitable (GrLivArea pequeña) y sótanos más pequeños.

- Muchas de estas casas tienen valores atípicos y precios significativamente más bajos en comparación con el resto del dataset.

- Posible ubicación en vecindarios menos costosos.

  Cluster 2 (Azul - Centro): Casas de Precio Medio y Tamaño Promedio

- Representa la mayor parte de las casas del dataset.

- casas tienen una calidad de construcción media y tamaños moderados de área habitable y sótano.

- Su precio está dentro de un rango intermedio y pueden pertenecer a vecindarios de costo medio.

  Cluster 3 (Verde - Derecha): Casas de Alto Precio y Gran Tamaño

- Corresponde a casas de gran tamaño, con alta calidad de construcción y amplios sótanos.

- Están en los rangos de precios más altos y probablemente se ubiquen en vecindarios exclusivos.

- Hay una mayor dispersión en este grupo, lo que indica que el precio puede variar significativamente dependiendo de otras características.

2. Hallazgos Clave

- El clustering sugiere que el precio de una casa está fuertemente influenciado por el tamaño y la calidad de la construcción.

- Existen claras diferencias entre los grupos, lo que confirma que segmentar los datos ayuda a comprender mejor el comportamiento del precio de las casas.

- Algunas casas en el Cluster 1 (rojo) están alejadas de su grupo principal, lo que indica posibles valores atípicos o características únicas que afectan su precio.

- El Cluster 3 (verde) muestra una dispersión mayor, lo que implica que en el segmento de casas más costosas, el precio depende de múltiples factores y no solo del tamaño.

3. Implicaciones para la Predicción del Precio

- Segmentar los datos por clusters antes de entrenar un modelo podría mejorar la precisión de la predicción.

- Las casas en el Cluster 1 podrían ser mejor modeladas con características diferentes a las de los Clusters 2 y 3.

- El precio de las casas más caras (Cluster 3) es más variable, lo que sugiere que factores adicionales como ubicación y acabados tienen un impacto significativo.

------------ Conclusiones del Análisis de Grupos ------------

1. Calidad de Construcción (OverallQual) y su Relación con el Precio

- El análisis de la calidad de construcción mostró que OverallQual es un factor determinante en el precio de las casas. Las casas con mejor calidad de materiales y acabados tienden a venderse a precios significativamente más altos en comparación con aquellas con menor calidad.

- Casas de calidad alta (OverallQual ≥ 8): Se encuentran en el rango de precios más elevados y muestran menor variabilidad en los valores de venta, lo que indica que la calidad superior de los materiales es un factor clave para mantener precios altos y relativamente estables.

- Casas de calidad media (OverallQual 5-7): Representan la mayor parte del dataset y presentan una amplia variabilidad en los precios. Esto sugiere que otros factores, como el vecindario o el tamaño, pueden influir en el precio final de estas viviendas.

- Casas de calidad baja (OverallQual ≤ 4): Tienen precios más bajos y menos variabilidad, lo que sugiere que hay menos factores adicionales que puedan incrementar su valor significativamente.

- Conclusión: La calidad de construcción es uno de los principales indicadores del precio de una casa. Casas con materiales y acabados de mejor calidad tienden a mantener precios más elevados y estables, mientras que aquellas con menor calidad están en un rango de precios más bajo y predecible.

2.  Ubicación (Neighborhood) y su Impacto en el Precio

- El vecindario donde se encuentra una casa juega un papel crucial en la determinación de su precio. Al analizar los datos, se encontró que algunos vecindarios tienen precios consistentemente más altos que otros, lo que indica que factores como la demanda, la proximidad a servicios y la seguridad influyen en la valoración de las viviendas.

-  Vecindarios de alto valor: StoneBr, NridgHt, NoRidge presentan los precios promedio más altos, con valores que superan los 300,000 dólares. Esto sugiere que son áreas más exclusivas, con mejor infraestructura, accesibilidad y servicios.

-  Vecindarios de valor medio: Zonas como Somerst, Timber, Veenker tienen precios intermedios, mostrando que tienen características atractivas, pero no al nivel de los vecindarios más costosos.

- Vecindarios de bajo valor: MeadowV, IDOTRR, BrDale presentan los precios más bajos, en muchos casos por debajo de 150,000 dólares, lo que podría indicar menor demanda, menor calidad en la infraestructura o menor acceso a servicios de calidad.

- Conclusión: La ubicación es un factor crítico en la valoración de una casa. Vivir en un vecindario de alto valor puede aumentar significativamente el precio de una propiedad, mientras que en áreas de menor demanda, las viviendas tienen un techo de precio más bajo.

3. Tamaño de la Casa (GrLivArea) y su Influencia en el Precio
El área habitable de una casa (GrLivArea) es otro factor clave para determinar su valor. El análisis de grupos basado en el tamaño de las viviendas mostró una clara relación entre el área total y el precio de venta, aunque con cierta variabilidad.

- Casas grandes (GrLivArea ≥ 2000 m²): Estas viviendas tienen los precios más altos, pero con una mayor dispersión en los valores. Esto sugiere que otros factores como la ubicación y la calidad de construcción pueden influir significativamente en su valoración.

- Casas medianas (1000 ≤ GrLivArea < 2000 m²): Representan la mayor parte del dataset y muestran una distribución amplia de precios. En este grupo, la influencia de otros factores como el vecindario y la calidad de los acabados es más evidente.

- Casas pequeñas (GrLivArea < 1000 m²): Son las de menor precio y presentan menos variabilidad en los valores de venta, lo que indica que hay menos margen para variaciones de precio en función de otros factores.

- Conclusión: Aunque el tamaño de la casa es un fuerte predictor del precio de venta, no es el único determinante. Casas de mayor tamaño tienden a costar más, pero la calidad de construcción y la ubicación pueden hacer que algunas casas pequeñas sean más valiosas que otras más grandes.

Conclusiones del Análisis de Clusters

1. El clustering confirmó los patrones que habíamos identificado manualmente.

- Las casas se dividen en segmentos de Bajo, Medio y Alto precio, con diferencias en tamaño y calidad.

2. El clustering puede ser usado como una nueva variable (Cluster) para mejorar la predicción de SalePrice.

- Agregar Cluster como variable categórica en el modelo de regresión podría mejorar el desempeño.

3. El modelo puede beneficiarse de una segmentación más avanzada.

Conclusión General del Análisis de Grupos

A través de este análisis, se ha identificado que las casas pueden agruparse en diferentes segmentos en función de su calidad de construcción, ubicación y tamaño.

- La calidad de los materiales y acabados (OverallQual) es uno de los principales indicadores del precio de una casa. Las viviendas con calificación alta en calidad tienden a mantener precios elevados y estables.

- El vecindario (Neighborhood) tiene un impacto significativo en el precio de venta. Las casas ubicadas en vecindarios de alta demanda y con mejor infraestructura tienden a costar más.

- El tamaño de la casa (GrLivArea) influye en el precio, pero su impacto puede estar moderado por la calidad y la ubicación.

### 4.	Divida el set de datos preprocesados en dos conjuntos: Entrenamiento y prueba. Describa el criterio que usó para crear los conjuntos: número de filas de cada uno, estratificado o no, balanceado o no, etc. Use el conjunto de datos llamado “train.csv”. Extraiga de ahí su  subconjunto de prueba. 

4. División del Dataset en Conjuntos de Entrenamiento y Prueba

Objetivo:

El propósito de esta división es separar los datos en un conjunto de entrenamiento (que se usará para ajustar el modelo) y un conjunto de prueba (que se usará para evaluar el desempeño del modelo).

Dado que estamos trabajando con datos de precios de casas, nos aseguramos de que la división sea aleatoria pero balanceada, de manera que la distribución de SalePrice en ambos conjuntos sea similar.

Criterios para la División:

- Dataset original: train.csv con 1460 registros.
- Tamaño del conjunto de entrenamiento: 80% de los datos (~1168 registros).
- Tamaño del conjunto de prueba: 20% de los datos (~292 registros).
- Método de selección: Muestreo aleatorio estratificado en base a SalePrice para mantener la misma distribución en ambos conjuntos.
- Balanceo: Se verificará que el conjunto de prueba tenga una distribución similar al conjunto de entrenamiento en términos de la variable SalePrice.

```{r}
# Cargar la librería necesaria
library(caret)

# Fijar semilla para reproducibilidad
set.seed(42)

# Crear índices para el conjunto de entrenamiento (80%)
trainIndex <- createDataPartition(train$SalePrice, p = 0.8, list = FALSE)

# Crear conjuntos de entrenamiento y prueba
train_set <- train[trainIndex, ]
test_set <- train[-trainIndex, ]

# Verificar tamaños
cat("Tamaño del conjunto de entrenamiento:", nrow(train_set), "\n")  # 1168 registros
cat("Tamaño del conjunto de prueba:", nrow(test_set), "\n")  # 292 registros

# Guardar los datasets en archivos CSV
write.csv(train_set, "train_set.csv", row.names = FALSE)
write.csv(test_set, "test_set.csv", row.names = FALSE)
```
Validación de la División

Para confirmar que la distribución de SalePrice es similar en ambos conjuntos, generamos histogramas:

```{r}
# Comparar la distribución de SalePrice en ambos conjuntos
ggplot(train_set, aes(x = SalePrice)) +
  geom_histogram(bins = 50, fill = "blue", alpha = 0.5) +
  labs(title = "Distribución de Precios en Entrenamiento")

ggplot(test_set, aes(x = SalePrice)) +
  geom_histogram(bins = 50, fill = "red", alpha = 0.5) +
  labs(title = "Distribución de Precios en Prueba")
```
Conclusiones sobre la División del Conjunto de Datos

La partición del dataset se realizó con el objetivo de garantizar que el conjunto de entrenamiento y el conjunto de prueba tengan una distribución representativa de los precios de las casas (SalePrice). Se utilizaron métodos de muestreo aleatorio estratificado para mantener el balance de los datos. A continuación, se presentan los hallazgos clave:

Análisis de la Distribución en Entrenamiento y Prueba

Los histogramas muestran la distribución de SalePrice en ambos conjuntos:

1. Distribución en el Conjunto de Entrenamiento

- Se observa una concentración de casas en el rango de 100,000 a 200,000 dólares, con algunos valores más altos que actúan como posibles outliers.

-La distribución tiene un sesgo positivo (asimétrica a la derecha), lo que sugiere la presencia de viviendas de muy alto valor.
-La mayoría de las casas están en un rango de precios accesibles, pero hay algunas con precios muy elevados que pueden afectar la regresión.

2. Distribución en el Conjunto de Prueba

- La forma de la distribución es similar a la del conjunto de entrenamiento, lo que indica que la partición mantuvo la estructura de los datos.

- La presencia de precios altos también se mantiene, aunque en menor cantidad debido al tamaño reducido del conjunto de prueba.

Validación de la División

- La partición se realizó de manera aleatoria pero estratificada, asegurando que la distribución de SalePrice en ambos conjuntos se mantenga lo más similar posible.

- El conjunto de entrenamiento contiene el 80% de los datos (~1168 registros), mientras que el conjunto de prueba tiene el 20% restante (~292 registros), lo que proporciona suficiente información para entrenar y evaluar el modelo.

- Los histogramas muestran que la distribución de precios en el conjunto de prueba es coherente con la del conjunto de entrenamiento, lo que indica que la partición no introduce sesgos significativos.

- El conjunto de prueba proviene directamente del dataset train.csv, asegurando que los datos sean representativos del problema que se quiere modelar.

Conclusión final: La división de los datos está bien estructurada y lista para ser utilizada en la construcción y evaluación del modelo de regresión.

### 5.	Haga ingeniería de características, ¿qué variables cree que puedan ser mejores predictores para el precio de las casas? Explique en que basó la selección o no de las variables. 

5. Ingeniería de Características: Selección de Variables Predictoras

- La ingeniería de características es un paso clave para mejorar el rendimiento del modelo de regresión. En este proceso, seleccionamos las variables más relevantes para predecir SalePrice, transformamos algunas características y descartamos aquellas que no aportan valor.

Criterios para la Selección de Variables

- Para determinar qué variables son las mejores predictoras del precio de las casas, consideramos los siguientes criterios:

5.1. Correlación con SalePrice

- Se identificaron variables con alta correlación positiva con el precio de las casas (OverallQual, GrLivArea, GarageCars, TotalBsmtSF, etc.).

- Se eliminaron variables con baja o nula correlación, como MiscFeature, PoolArea, LowQualFinSF.

5.2.  Relevancia en el Dominio del Problema

- Factores como calidad de construcción, ubicación y tamaño de la vivienda son críticos en la valoración de una casa.

- Características como el vecindario (Neighborhood) pueden influir significativamente en el precio debido a la oferta y demanda inmobiliaria.

5.3.  Valores Faltantes y Redundancia

- Variables con demasiados valores faltantes (más del 80%) fueron eliminadas (PoolQC, Alley, MiscFeature).
Se eliminaron variables redundantes (por ejemplo, GarageCars y GarageArea están fuertemente correlacionadas, por lo que solo se conserva una de ellas).

5.4. Variables Seleccionadas

1. Variables Numéricas Fuertes Predictoras (Correlación Alta con SalePrice)

- OverallQual (0.79) → Calidad general de la construcción.
- GrLivArea (0.71) → Área habitable sobre el suelo.
- GarageCars (0.64) → Número de autos en el garaje.
- TotalBsmtSF (0.61) → Área total del sótano.
- 1stFlrSF (0.59) → Área del primer piso.
- FullBath (0.56) → Número de baños completos.
- TotRmsAbvGrd (0.53) → Total de habitaciones sobre el suelo.

2. Variables Categóricas Importantes

- Neighborhood → Diferencias significativas en los precios de las casas según la ubicación.
- HouseStyle → El tipo de casa influye en el valor de la propiedad.
- ExterQual → Calidad de los materiales exteriores.
- BsmtQual → Calidad del sótano, importante para valorar el espacio utilizable.
- GarageType → Tipo de garaje afecta la funcionalidad y valor de la vivienda.
- SaleCondition → Tipo de venta puede influir en los precios (ejemplo: ventas normales vs. subastas).

5.5. Transformaciones y Creación de Nuevas Variables

Para mejorar el modelo, realizamos algunas transformaciones y creamos nuevas características:

1.  Transformación Logarítmica de SalePrice

- Como la variable SalePrice tiene un sesgo positivo (asimétrico a la derecha), aplicamos una transformación logarítmica para normalizarla:

```{r}
train$LogSalePrice <- log(train$SalePrice)
```

2. Codificación de Variables Categóricas

- Convertimos variables categóricas en factores para que sean utilizables en modelos de regresión:

```{r}
train <- train %>% mutate(across(where(is.character), as.factor))
test <- test %>% mutate(across(where(is.character), as.factor))
```

3. Feature Engineering: Crear Nueva Variable Age

- En lugar de usar YearBuilt y YearRemodAdd, creamos una variable Age que representa la edad de la casa en el momento de la venta:

```{r}
train$Age <- train$YrSold - train$YearBuilt
```

4. Interacción entre Variables

- Calidad y Tamaño Combinados: Creamos una nueva variable Qual_LivArea, que combina la calidad de la construcción con el área habitable:

```{r}
train$Qual_LivArea <- train$OverallQual * train$GrLivArea
```

Variables Eliminadas

- Variables con más del 80% de valores faltantes: PoolQC, MiscFeature, Alley, Fence, FireplaceQu.
- Variables con baja correlación con SalePrice: MiscVal, 3SsnPorch, ScreenPorch, MoSold, YrSold.
- Variables redundantes: GarageArea (se queda GarageCars porque tiene mayor correlación con SalePrice).

- Conclusiones sobre la Ingeniería de Características

- Se seleccionaron variables con alta correlación con SalePrice, asegurando que el modelo tenga predictores relevantes.

- Se eliminaron variables irrelevantes o con valores faltantes excesivos, reduciendo el ruido en los datos.

- Se crearon nuevas variables (LogSalePrice, Age, Qual_LivArea) para mejorar la capacidad predictiva del modelo.

- Las variables categóricas fueron codificadas, permitiendo su uso en modelos de regresión.

- Con esta selección y transformación de características, los datos están listos para entrenar un modelo de regresión de manera más efectiva.

### 6.	Todos los resultados deben ser reproducibles por lo que debe fijar que los conjuntos de entrenamiento y prueba sean los mismos siempre que se ejecute el código. 

6️. Asegurar Reproducibilidad de los Resultados

- Para garantizar que los conjuntos de entrenamiento y prueba sean los mismos en cada ejecución, debemos fijar una semilla aleatoria (set.seed()) en R. Esto asegura que la partición de los datos se realice siempre de la misma manera.

```{r}
# Cargar librerías necesarias
library(caret)
library(dplyr)
library(ggplot2)

# Fijar semilla para reproducibilidad
set.seed(42)

# Crear índices para el conjunto de entrenamiento (80%) y prueba (20%)
trainIndex <- createDataPartition(train$SalePrice, p = 0.8, list = FALSE)

# Crear conjuntos de entrenamiento y prueba
train_set <- train[trainIndex, ]
test_set <- train[-trainIndex, ]

# Verificar tamaños
cat("Tamaño del conjunto de entrenamiento:", nrow(train_set), "\n")  # 1168 registros
cat("Tamaño del conjunto de prueba:", nrow(test_set), "\n")  # 292 registros

# Guardar los datasets para que siempre sean los mismos
write.csv(train_set, "train_set.csv", row.names = FALSE)
write.csv(test_set, "test_set.csv", row.names = FALSE)
```
¿Cómo Funciona esto?

- set.seed(42): Fija la semilla aleatoria para que la partición de los datos sea siempre la misma en cada ejecución.

- createDataPartition(): Realiza la partición estratificada, asegurando que la distribución de SalePrice sea similar en ambos conjuntos.

- Se guardan los conjuntos en archivos CSV (train_set.csv y test_set.csv), para que siempre se pueda cargar la misma partición sin necesidad de recalcularla.

Validación de Reproducibilidad

Cada vez que se ejecute este código:

- La partición de datos será idéntica en todas las ejecuciones.

- Los histogramas de SalePrice en los conjuntos de entrenamiento y prueba se mantendrán constantes.

- Los modelos entrenados con estos conjuntos siempre darán los mismos resultados.

--- Con esto, garantizamos que los resultados sean 100% reproducibles.

### 7.	Seleccione una de las variables y haga un modelo univariado de regresión lineal para predecir el precio de las casas. Analice el modelo (resumen, residuos, resultados de la predicción). Muéstrelo gráficamente. 

7. Modelo Univariado de Regresión Lineal para Predecir SalePrice

Selección de Variable
Para construir un modelo univariado de regresión lineal, seleccionamos la variable con mayor correlación con SalePrice.

Según el análisis previo, la variable OverallQual (calidad general de la construcción) tiene la correlación más alta con SalePrice (0.79), por lo que la usaremos para el modelo.

Construcción del Modelo Univariado...

```{r}
# Cargar librerías necesarias
library(ggplot2)

# Ajustar el modelo de regresión lineal univariado
lm_model <- lm(SalePrice ~ OverallQual, data = train_set)

# Resumen del modelo
summary(lm_model)
```
Análisis del Modelo
El comando summary(lm_model) nos proporciona información clave:

1. Intercepto y Coeficiente

- El coeficiente de OverallQual indica cuánto aumenta SalePrice por cada unidad de mejora en calidad de construcción.

- Si el coeficiente es positivo y significativo (p-value < 0.05), entonces OverallQual tiene una relación lineal con SalePrice.

2. R^2 Ajustado (Coeficiente de Determinación)

- Indica qué porcentaje de la variabilidad en SalePrice es explicada por OverallQual.

- Si es alto (>0.60), significa que OverallQual es una buena predictor.

3. Significancia del Modelo

- Se revisa el p-value para ver si el modelo es estadísticamente significativo.

Análisis de Residuos..

```{r}
# Diagnóstico de residuos
par(mfrow = c(2, 2))  # Crear un panel de 2x2 gráficos
plot(lm_model)
```
¿Qué buscamos en los residuos?

- Normalidad: El gráfico Q-Q debe seguir una línea recta.

- Homocedasticidad: Los residuos deben estar distribuidos aleatoriamente alrededor de 0.

- Ausencia de patrones en los residuos: Si hay un patrón curvado, la relación no es completamente lineal.

Visualización del Modelo...

```{r}
# Cargar librerías necesarias
library(ggplot2)

# Gráfico de dispersión con línea de regresión
ggplot(train_set, aes(x = OverallQual, y = SalePrice)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", col = "red") +
  labs(title = "Regresión Lineal: SalePrice vs OverallQual",
       x = "Calidad General de Construcción (OverallQual)",
       y = "Precio de Venta (SalePrice)")
```
¿Qué muestra este gráfico?

- La relación entre OverallQual y SalePrice (si es lineal y creciente).

- Si hay outliers que pueden afectar la predicción.

Predicción en el Conjunto de Prueba...

```{r}
# Predecir precios en el conjunto de prueba
test_set$Predicted_SalePrice <- predict(lm_model, newdata = test_set)

# Comparar los valores reales vs. predichos
ggplot(test_set, aes(x = SalePrice, y = Predicted_SalePrice)) +
  geom_point(alpha = 0.5, color = "blue") +
  geom_abline(slope = 1, intercept = 0, col = "red", linetype = "dashed") +
  labs(title = "Comparación: Precio Real vs Predicho",
       x = "Precio Real",
       y = "Precio Predicho")
```
Análisis de Resultados del Modelo Univariado de Regresión Lineal

- La gráfica muestra la relación entre el precio real (SalePrice) y el precio predicho por el modelo utilizando OverallQual como única variable predictora.

Observaciones Clave
Tendencia general positiva:

- Existe una correlación positiva entre el precio real y el predicho.

- Los valores más altos de SalePrice tienden a ser predichos en el rango correcto.

Problemas en la predicción:

- Alta dispersión: Muchos puntos se alejan de la línea roja (que representa una predicción perfecta).
Predicciones en grupos discretos: Esto ocurre porque OverallQual es una variable discreta (categorías de 1 a 10), por lo que el modelo solo predice ciertos valores en escalones.

- Subestimación de precios altos: Se observa que para casas más caras, el modelo tiende a predecir valores más bajos de los reales.

Conclusiones

1. OverallQual es una buena variable predictora, pero no suficiente por sí sola.

- Explica parte de la variabilidad de SalePrice, pero hay otros factores importantes no considerados.

2. Se necesita un modelo multivariado.

- Otras variables como GrLivArea, TotalBsmtSF, y GarageCars deben incluirse para mejorar la predicción.

3. El modelo univariado es útil como punto de partida, pero no es suficiente para hacer predicciones precisas.

Por lo tanto:

1. El modelo univariado de regresión lineal muestra que OverallQual tiene una fuerte influencia en SalePrice, pero no explica toda la variabilidad.

2. El R^2 ajustado nos indica qué tan bueno es el ajuste del modelo.

3. El análisis de residuos nos ayuda a verificar si los supuestos de regresión lineal se cumplen.

4. La predicción en el conjunto de prueba nos muestra la capacidad del modelo para generalizar.

Este modelo es un punto de partida, pero un modelo multivariable será más preciso para predecir SalePrice.

### 8.	Haga un modelo de regresión lineal con todas las variables numéricas para predecir el precio de las casas. Analice el modelo (resumen, residuos, resultados de la predicción). Muestre el modelo gráficamente.

- Lo que vamos a hacer es crear con todos los predictores de train , pero antes vamos a convertirlos a numericas todas.

```{r}


train <- read.csv("./train_set.csv")
train[] <- lapply(train, function(x) {
  if (is.factor(x) || is.character(x)) {
    as.numeric(as.factor(x))
  } else {
    x  
  }
})




test_d <- read.csv("./test_set.csv")


test_d[] <- lapply(test_d, function(x) {
  if (is.factor(x) || is.character(x)) {
    as.numeric(as.factor(x))
  } else {
    x  
  }
})

setdiff(names(train), names(test_d))  # Debería mostrar character(0) si ya son iguales
setdiff(names(test_d), names(train))  # Debería mostrar character(0) si ya son iguales
```

- Miramos la distribucion de SalesPrice
```{r}
paquetes_necesarios <- c("ggcorrplot", "ggplot2")

paquetes_faltantes <- paquetes_necesarios[!(paquetes_necesarios %in% installed.packages()[,"Package"])]

if(length(paquetes_faltantes) > 0) {
  install.packages(paquetes_faltantes, dependencies = TRUE)
}

library(ggplot2)

ggplot(train, aes(x = SalePrice)) + 
  geom_histogram(bins = 50, fill = "blue", alpha = 0.7) +
  labs(title = "Distribución de Precios de Casas", x = "Precio de Venta", y = "Frecuencia")
```

- Aplicamos logaritmo para quitar el sesgo
```{r}
train$SalePrice <- log(train$SalePrice)
test_d$SalePrice<- log(test_d$SalePrice)
ggplot(train, aes(x = SalePrice)) +
  geom_histogram(bins = 50, fill = "red", alpha = 0.7) +
  labs(title = "Distribución Logarítmica de los Precios", x = "Log(Precio)", y = "Frecuencia")
```


- Ya con esto creamos nuestro modelo con todas las variables.

```{r}
library(dplyr)
train <- train %>% select(-any_of(c("Id", "Cluster")))

test_d <- test_d %>% select(-any_of(c("Id", "Cluster")))


modelo1 <- lm(SalePrice~.,data = train)
summary(modelo1)
```


#### Analisis

- Podemos ver que nuestro modelo tiene un R^2 de 0.99 lo que es una muy buena señal de prediccion ya que explica el 99% de los datos.Ademas que el F stadistics indica que es altamente significativo ya que su p value es muy pequeño.

- TotalBsmtSF, GrLivArea, Age estan correlacionadas con otras, y se elimino de este modelo de manera automatica.

- Las variables significativas fueron 
OverallQual (p < 2e-16, coef. = 0.0006618) → La calidad general de la casa es el predictor más fuerte.
1stFlrSF (p < 2e-16, coef. = 2.766e-06) → El tamaño del primer piso tiene un efecto positivo importante.
2ndFlrSF (p < 2e-16, coef. = 2.704e-06) → El área del segundo piso también impacta en el precio.
Qual_LivArea (p < 2e-16, coef. = -4.112e-07) → Una métrica combinada de calidad y área afecta el precio.
LotFrontage (p = 6.07e-08, coef. = 4.972e-06) → El ancho del lote es un predictor relevante.
MasVnrArea (p = 0.000192, coef. = -4.073e-07) → El área de mampostería tiene una relación negativa con SalePrice.
ExterQual (p = 0.000415, coef. = 1.291e-04) → La calidad del exterior es un factor clave.
HeatingQC (p = 0.002198, coef. = -3.493e-05) → La calidad de la calefacción influye significativamente.
CentralAir (p = 2.85e-12, coef. = 6.597e-04) → Tener aire acondicionado central impacta fuertemente en el precio.

- Las no significativas fueron

Street (p = 0.148)
BsmtCond (p = 0.631)
Fireplaces (p = 0.968)
GarageType (p = 0.687)



El problema de nuestro modelo es que es muy sobreajustado. Asi que vamos a ver si esta sobreajustado

```{r residuosModelo1}
plot(modelo1)
```
#### Analisis

Se puede ver que la varianza es constante, de hecho hay una tendencia a esta msima. Y se observa un patron en el grafico de residuales y muestra que la relacion no es completamete lineal, De hecho hay puntos de los mismos que deben analizarse porque puede ser que esto afecte la variabilidad del modelo y lo haga menos generalista. De hecho se ve en el grafico qq que no hay una tendencia lineal de los datos.  Para ello haremos un un test de lilliefors si los residuos se distribuyen normalmente.  
```{r lilliefors residuos modelo 1}
library(nortest)

lillie.test(modelo1$residuals)
```

El p-valor es menor que 0.05 por lo que se rechaza la hipótesis nula de normalidad de los datos. Los residuos no están distribuidos normalmente.   


Ahora lo que haremos es normalizarlos para ello realizamos el mismo procedimiento solo que ahora


#### Normalizando los datos
  
Dado que hay diferencias de escala en las variables por lo que vamos a normalizar los datos y hacer un modelo para ver si resulta mejor modelo

```{r normalizando train y test}
train_normal <- as.data.frame(scale(train))
```

```{r modelo 1}
modelo1.1<-lm(SalePrice~.,data = train_normal)
summary(modelo1.1)
```

El modelo normalizado explica el 0.9997 de los datos.

```{r}
plot(modelo1.1)
```
#### Analisis Grafico
Parece haber una leve tendencia no lineal (curva roja), lo que indica que la relación entre las variables podría no ser completamente lineal.

Los residuales tienen una varianza no constante, es posible que se requieran correcciones como regresión ponderada o transformación de variables.

Se ve tambien una mejoria en el grafico q q en donde ahora vemos que sigue la curva la mayoria de variables aunque siguen habiendo varias que se desvian de la original. 

Si verificamos con lillie test

```{r}
library(nortest)
lillie.test(modelo1.1$residuals)
```
El pvalue sigue siendo mas pequeño a 0.05 lo que nos indica que tampoco sirvio normalizar. 


Lo  que vamos a hacer es seleccion de predictores con stepwise.

```{r stepwise}
# Eliminar filas con valores NA
train_clean <- na.omit(train)

# Ajustar modelo con stepwise backward
modelo2 <- step(
    object    = lm(formula = SalePrice ~ ., data = train_clean),
    direction = "backward",
    scope     = list(upper = ~., lower = ~1),
    trace     = FALSE
)
summary(modelo2)
```


Ahora vamos a ver los residuos. 

```{r}
plot(modelo2)
```
#### Analisis 

Podemos ver que ha mejorado un poco en la manera en como se muestran los residuos, Pero siguen habiendo valores atipicos aun con eliminar varios que dijimos que con stepwise se mejoraria. 

```{r normalidad de residuos modelo2}
library(nortest)
lillie.test(modelo2$residuals)
```

Sigue sin mejorar aun usando el stepwise. Por lo que debemos de analizar la multicolinealidad de las variables y ver cuales debemos de seleccionar. Pero antes miremos como predice este modelo

```{r}


test_d[is.na(test_d)] <- 0  
for (col in names(test_d)) {
  test_d[[col]][is.na(test_d[[col]])] <- median(test_d[[col]], na.rm = TRUE)
}



predicciones_train <- predict(modelo1, newdata = train)

predicciones_test <- predict(modelo1, newdata = test_d)

predicciones_test_original <- exp(predicciones_test)


```


```{r}
faltantes <- setdiff(names(train), names(test_d))
for (col in faltantes) {
  test_d[[col]] <- NA
}


plot(test_d$SalePrice,col="blue", main="Predicciones vs valores originales")
points(predicciones_test, col="red")
legend(30,45,legend=c("original", "prediccion"),col=c("blue", "red"),pch=1, cex=0.8)


```


#### Analisis

Al final el modelo 1 no es muy predictivo y al ver los siguientes solo estamos haciendo que sea menos predictivo. Por ello mismo lo que vamos a hacer es utilizar otros metodos para poder eliminar  variables y reducir nuestro overfitting del modelo. Pero antes miremos la multicolineadlidad de los datos.




### 9.	Analice el modelo. Determine si hay multicolinealidad entre las variables, y cuáles son las que aportan al modelo. Haga un análisis de correlación de las características del modelo y especifique si el modelo se adapta bien a los datos. Explique si hay sobreajuste (overfitting) o no. En caso de existir sobreajuste, haga otro modelo que lo corrija. 

Podemos ver que nuestro modelo lastimosamente si esta sobreajustado ya que su R ajustado es de 0.97 y de 1 si realizamos stepwise . Ademas que podemos ver que al predecir con los test no lo esta haciendo de la manera correcta aun cuando normalicemos , y apliquemos tecnicas para reducir datos atipicos.

- Lo primero que vamos a hacer es usar la matriz de correlacion. 

```{r}
library(car)


cor_matrix <- cor(train[, -which(names(train) == "SalePrice")], use = "pairwise.complete.obs")


library(corrplot)
corrplot(cor_matrix, method = "color", tl.cex = 0.6, tl.srt = 90)



```


Ahora conocemos la correlacion alta

```{r}

high_cor <- which(abs(cor_matrix) > 0.8, arr.ind = TRUE)


high_cor <- high_cor[high_cor[,1] != high_cor[,2], ]
print(high_cor)
```

Como podemos ver estas variables estan causando alta coinealidad de hecho podemos graficarlas con una matriz de correlacion

```{r}

top_vars <- c("LogSalePrice", "Qual_LivArea", "GarageYrBlt", "Age", "Exterior2nd", 
              "Exterior1st", "X1stFlrSF", "TotalBsmtSF", "TotRmsAbvGrd", "SizeGroup", 
              "GrLivArea", "YearBuilt", "OverallQual")


filtered_train <- train[, top_vars]


cor_matrix_filtered <- cor(filtered_train, use = "pairwise.complete.obs")


library(corrplot)
corrplot(cor_matrix_filtered, method = "color", type = "upper",
         tl.cex = 0.8, tl.srt = 45,
         col = colorRampPalette(c("blue", "white", "red"))(200),
         cl.cex = 0.8)


```



Pero antes de eliminar variables multicolineales vamos a hacer una cosa. Utilizaremos Ridge y regularicemos para ver que tan bien rinden.

```{r}

x_train <- model.matrix(SalePrice~., data = train)
y_train <- train$SalePrice

x_test <- model.matrix(SalePrice~., data = test_d)
y_test <- test_d$SalePrice

```

```{r}
# Asegurar que ambos tengan las mismas filas



set.seed(123)  # Para reproducibilidad
indices <- sample(1:nrow(x_train), 1064)  # Seleccionar 1064 índices aleatorios

x_train <- x_train[indices, ]  # Filtrar filas de x_train
y_train <- y_train[indices]  # Filtrar elementos de y_train



# Variables en train y test
vars_train <- colnames(x_train)
vars_test <- colnames(x_test)

# Variables que están en test pero no en train
extra_vars_in_test <- setdiff(vars_test, vars_train)
print(extra_vars_in_test)
x_test <- x_test[, colnames(x_train)]


```

Se hace la regularización con un 100 números de λ porque no sabemos cual es el lambda adecuado.
```{r}
library(tidyverse)
library(glmnet)
modelo3 <- glmnet(
            x           = x_train,
            y           = y_train,
            alpha       = 0,
            nlambda     = 100,
            standardize = TRUE
          )

regularizacion <- modelo3$beta %>% 
                  as.matrix() %>%
                  t() %>% 
                  as_tibble() %>%
                  mutate(lambda = modelo3$lambda)

regularizacion <- regularizacion %>%
                   pivot_longer(
                     cols = !lambda, 
                     names_to = "predictor",
                     values_to = "coeficientes"
                   )
```
 Ahora graficamos loos coeficientes
 
```{r}
 regularizacion %>%
  ggplot(aes(x = lambda, y = coeficientes, color = predictor)) +
  geom_line() +
  labs(title = "Coeficientes del modelo en función de la regularización") +
  theme_bw() +
  theme(legend.position = "none")
 
```
 
#### Analisis

Como podemos ver el valor de lambda indica que mientras mas pequeños los coeficientes mayor es lambda y mientras mas grande los coeficientes mayor el lambda.


Ahora obtendremos el valor optimo de este con la validacion cruzada

```{r}
cv_error <- cv.glmnet(
              x      = x_train,
              y      = y_train,
              alpha  = 0,
              nfolds = 10,
              type.measure = "mse",
              standardize  = TRUE
           )

plot(cv_error)

lambda_opt <- cv_error$lambda.min
print(lambda_opt)
```

Vemos que el valor es de 26.461 asi que con estos nuevos datos vamos a entrenar nuestro modelo

```{r}
modelo3 <- glmnet(
            x           = x_train,
            y           = y_train,
            alpha       = 0,
            lambda      = cv_error$lambda.1se,
            standardize = TRUE
          )
```

Ahora vemos los predictores y sus coeficientes. Ridge no elimina variables 

```{r}
coef(modelo3)
 
```
```{r}



predicciones_train_modelo3<- predict(modelo3, newx = x_train)


predicciones_test_modelo3 <- predict(modelo3, newx = x_test)



y_test <- y_test[1:nrow(x_test)]

# MSE de test
test_mse_ridge_modelo3 <- mean((predicciones_test_modelo3 - y_test)^2)

# MSE de entrenamiento
training_mse_ridge_modelo3 <- mean((predicciones_train_modelo3 - y_train)^2)


print(test_mse_ridge_modelo3)
print(training_mse_ridge_modelo3)
``` 
#### Analisis

Ridge muestra que el modelo quedo bastante bien, aunque puede ser no muy bueno debido a que uno muy bajo indica sobreajuste. Pero sabemos por teoria que Ridge solo elimina sobreajuste acercando valores a 0 pero no eliminandolos que es lo que vimos en el grafico de los lambdas encontrados. 



```{r}

predicciones_train_modelo3 <- predict(modelo3, newx = x_train)
predicciones_test_modelo3 <- predict(modelo3, newx = x_test)


y_test <- y_test[1:nrow(x_test)]

# Calcular MSE
test_mse_ridge_modelo3 <- mean((predicciones_test_modelo3 - y_test)^2)
training_mse_ridge_modelo3 <- mean((predicciones_train_modelo3 - y_train)^2)

# Calcular residuos
residuos_train <- y_train - predicciones_train_modelo3
residuos_test <- y_test - predicciones_test_modelo3


print(test_mse_ridge_modelo3)
print(training_mse_ridge_modelo3)
summary(residuos_train)
summary(residuos_test)


par(mfrow=c(2,2))  


hist(residuos_train, main="Histograma de residuos (Train)", 
     col="lightblue", breaks=30, xlab="Residuos", ylab="Frecuencia")


hist(residuos_test, main="Histograma de residuos (Test)", 
     col="lightgreen", breaks=30, xlab="Residuos", ylab="Frecuencia")



plot(predicciones_train_modelo3, residuos_train, 
     main="Residuos vs Predicciones (Train)", 
     xlab="Predicciones", ylab="Residuos", pch=20, col="blue")
abline(h=0, col="red", lwd=2)


plot(predicciones_test_modelo3, residuos_test, 
     main="Residuos vs Predicciones (Test)", 
     xlab="Predicciones", ylab="Residuos", pch=20, col="green")
abline(h=0, col="red", lwd=2)

# Resetear layout
par(mfrow=c(1,1))

``` 

#### Analisis

Podemos ver que a diferencia del anterior que habiamos hecho con Stepwise ahora si vemos que los residuos ya no tienen un patron o forma clara. Lo que indica que el modelo esta haciendo buena estimacion de los valores que deberia tener. Lo que indica que podria ser un muy buen modelo . Aunque se puede ver que el histograma de Residuos esta sesgado a la derecha , posiblemente estamos viendo un valor atipico a la izquierda. O un desbalance de nuestras predicciones en test.



### 10.	Si tiene multicolinealidad o sobreajuste, haga un modelo con las variables que sean mejores predictoras del precio de las casas. Determine la calidad del modelo realizando un análisis de los residuos. Muéstrelo gráficamente. 


Viendo en stepwise y al normalizar si hay sobreajuste asi que vamos a realizar Lazz y vamos a eliminar aquellas que sean peores predictoras o que no estan aportando a la prediccion de resultados. 

Para lazoo necesitaremos alpha

```{r}
modelo4 <- glmnet(
            x           = x_train,
            y           = y_train,
            alpha       = 1,
            nlambda     = 100,
            standardize = TRUE
          )

regularizacion <- modelo4$beta %>% 
                  as.matrix() %>%
                  t() %>% 
                  as_tibble() %>%
                  mutate(lambda = modelo4$lambda)

regularizacion <- regularizacion %>%
                   pivot_longer(
                     cols = !lambda, 
                     names_to = "predictor",
                     values_to = "coeficientes"
                   )

regularizacion %>%
  ggplot(aes(x = lambda, y = coeficientes, color = predictor)) +
  geom_line() +
  labs(title = "Coeficientes del modelo en función de la regularización") +
  theme_bw() +
  theme(legend.position = "none")
``` 
#### Analisis
Este grafico indica lo mismo que con Ridge solo que vemos que en muy pocos valores de lambda estamos viendo que los coeficientes bajen abruptamente . 


Ahora veamos el mejor valor de lambda con validacion cruzada
```{r}
cv_error <- cv.glmnet(
              x      = x_train,
              y      = y_train,
              alpha  = 1,
              nfolds = 10,
              type.measure = "mse",
              standardize  = TRUE
           )

plot(cv_error)
lambda_opt2 <- cv_error$lambda.min
print(lambda_opt)
``` 

#### Analisis

Vemos que el valor mas optimo sigue siendo el mismo que el anterior de 26 de hecho hasta podemos ver que con respecto a los coeficientes no cambia tanto.


```{r}
modelo4 <- glmnet(
            x           = x_train,
            y           = y_train,
            alpha       = 1,
            lambda      = cv_error$lambda.1se,
            standardize = TRUE
          )
coef(modelo4)
``` 


Al final solo  nos quedamos con 2 esto es normal para este tipo de seleccion de variables

```{r}
predicciones_train_modelo4<- predict(modelo4, newx = x_train)


predicciones_test_modelo4 <- predict(modelo4, newx = x_test)

# MSE de test
test_mse_lasso_modelo4 <- mean((predicciones_test_modelo4 - y_test)^2)

# MSE de entrenamiento
training_mse_lasso_modelo4 <- mean((predicciones_train_modelo4 - y_train)^2)

print(test_mse_lasso_modelo4 )
print(training_mse_lasso_modelo4 )
``` 
#### Analisis

Podemos ver que realmente no cambio nada de los valores que se detectaron al inicio , asi que vamos ahora a mostrarlos en un grafico que demuestre la diferencia entre los modelos.



### 11.	Utilice cada modelo con el conjunto de prueba y determine la eficiencia del algoritmo para predecir el precio de las casas. ¿Qué tan bien lo hizo?¿Qué medidas usó para determinar la calidad de la predicción? 

### 12.	Discuta sobre la efectividad de los modelos. ¿Cuál lo hizo mejor? ¿Cuál es el mejor modelo para predecir el precio de las casas? Haga los gráficos que crea que le pueden ayudar en la discusión. 

